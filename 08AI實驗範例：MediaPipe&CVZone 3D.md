## 1. Google MediaPipe æ©Ÿå™¨å­¸ç¿’æ¡†æž¶
å®˜æ–¹ç¶²å€ï¼š[MediaPipe Solutions](https://ai.google.dev/edge/mediapipe/solutions/)
**Google MediaPipe** æ˜¯ Google å…¬å¸æ–¼ **2019 å¹´**æŽ¨å‡ºçš„é–‹æºæ©Ÿå™¨å­¸ç¿’æ¡†æž¶ã€‚
ä¸»è¦é‡å° **é›»è…¦è¦–è¦ºï¼ˆComputer Visionï¼‰** é ˜åŸŸï¼Œæä¾›å¿«é€Ÿä¸”è·¨å¹³å°çš„è§£æ±ºæ–¹æ¡ˆã€‚
ç”¨ **æ©Ÿå™¨å­¸ç¿’ç®¡ç·šï¼ˆML Pipelineï¼‰** çš„æ¦‚å¿µï¼Œä¾†è™•ç†è¤‡é›œä»»å‹™ã€‚
    
### 1.1. æ©Ÿå™¨å­¸ç¿’ç®¡ç·šï¼ˆML Pipelineï¼‰
æ©Ÿå™¨å­¸ç¿’ç®¡ç·šï¼šä¸€å€‹è®“ç³»çµ±è‡ªå‹•åŒ–ç”¢ç”Ÿ ML æ¨¡åž‹çš„ä½œæ¥­æµç¨‹ã€‚
é¡žä¼¼ã€Œç³»çµ±é–‹ç™¼ç”Ÿå‘½é€±æœŸï¼ˆSDLCï¼‰ã€çš„æ¦‚å¿µï¼Œä½†æ˜¯é‡å°æ©Ÿå™¨å­¸ç¿’çš„ç‰ˆæœ¬ã€‚
ç‰¹é»žï¼šå¼·èª¿ç‰ˆæœ¬æŽ§åˆ¶ã€è‡ªå‹•æ¸¬è©¦ã€åè¦†è¿­ä»£ï¼ˆIterative Cycleï¼‰ã€‚
æ¶‰åŠæ­¥é©Ÿï¼š
- è³‡æ–™è’é›†
- è³‡æ–™é è™•ç†
- æ¨¡åž‹è¨“ç·´èˆ‡è©•ä¼°

### 1.2. Labï¼šåœ¨æ¨¹èŽ“æ´¾å®‰è£ MediaPipe
```bash
pip install mediapipe==0.10.14
```
### 1.3. Labï¼šMediaPipeåœ–ç‰‡äººè‡‰åµæ¸¬



åŸºç¤Žæ¨¡åž‹ï¼šä½¿ç”¨ BlazeFace æ¨¡åž‹
- ç”± Google è‡ªè¡Œé–‹ç™¼
- åŸºæ–¼ Single Shot Detector (SSD) æž¶æ§‹ï¼Œé‡å°ç§»å‹•è£ç½®èˆ‡è¼•é‡ç´šæ‡‰ç”¨å„ªåŒ–è¨­è¨ˆ
åŠŸèƒ½ç‰¹é»žï¼š
- è¶…å¿«é€Ÿåµæ¸¬äººè‡‰
- æ¨¡åž‹è¼•é‡ã€é‹ç®—é€Ÿåº¦å¿«
- å¯ä»¥åœ¨åœ–ç‰‡æˆ–å½±ç‰‡ä¸­åµæ¸¬å‡ºå¤šå¼µäººè‡‰
- åŒæ™‚æ¨™ç¤ºå‡ºæ¯å¼µäººè‡‰çš„6å€‹é—œéµé»žï¼ˆKey Pointsï¼‰

MediaPipe äººè‡‰åµæ¸¬å›žå‚³è³‡æ–™
- åµæ¸¬åˆ°çš„äººè‡‰çš„çŸ©å½¢ç¯„åœåº§æ¨™ï¼ˆbounding boxï¼‰
- åµæ¸¬åˆ°çš„äººè‡‰ä¸Šçš„6å€‹é—œéµé»žåº§æ¨™ï¼šå·¦çœ¼ã€å³çœ¼ã€é¼»å°–ã€å˜´å·´ã€å·¦è€³ã€å³è€³


```python
import cv2
import mediapipe as mp

# åˆå§‹åŒ– MediaPipe æ¨¡çµ„
mp_face_detection = mp.solutions.face_detection
mp_drawing = mp.solutions.drawing_utils
face_detection = mp_face_detection.FaceDetection(
                      min_detection_confidence=0.5)

# è®€å–åœ–ç‰‡
img = cv2.imread("./image03.png")
if img is None:
    print("åœ–ç‰‡è®€å–å¤±æ•—ï¼Œè«‹æª¢æŸ¥è·¯å¾‘ï¼")
    exit()

# åµæ¸¬äººè‡‰
results = face_detection.process(img)

# ç•«å‡ºåµæ¸¬çµæžœ
if results.detections:
    for detection in results.detections:
        mp_drawing.draw_detection(img, detection)

# å„²å­˜åœ–ç‰‡
cv2.imwrite("./resulted_image.jpg", img)
```


![upgit_20250422_1745331984.png|324x373](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2025/04/upgit_20250422_1745331984.png)

### 1.4. Labï¼šMediaPipeåœ–ç‰‡è‡‰éƒ¨ç¶²æ ¼
```python
import cv2
import mediapipe as mp

mp_drawing = mp.solutions.drawing_utils
mp_face_mesh = mp.solutions.face_mesh
drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)
face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5,
                                  min_tracking_confidence=0.5)

img = cv2.imread("./image03.png")
results = face_mesh.process(img)
if results.multi_face_landmarks:
    for face_landmarks in results.multi_face_landmarks:
        mp_drawing.draw_landmarks(image=img,
                   landmark_list=face_landmarks,
                   connections=mp_face_mesh.FACEMESH_CONTOURS,
                   landmark_drawing_spec=drawing_spec,
                   connection_drawing_spec=drawing_spec)


# å„²å­˜åœ–ç‰‡
cv2.imwrite("face_detected.jpg", img)
print("åµæ¸¬å¾Œçš„åœ–ç‰‡å·²å„²å­˜åˆ° images/face_detected.jpg")
```


![upgit_20250422_1745332325.png|437x503](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2025/04/upgit_20250422_1745332325.png)

### 1.5. Labï¼šMediaPipeåœ–ç‰‡æ‰‹å‹¢è¿½è¹¤
```python
import cv2
import mediapipe as mp

mp_drawing = mp.solutions.drawing_utils
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(min_detection_confidence=0.5,min_tracking_confidence=0.5)

img = cv2.imread("./hand.jpg")
results = hands.process(img)
if results.multi_hand_landmarks:
    for hand_landmarks in results.multi_hand_landmarks:
        mp_drawing.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)

# å„²å­˜åœ–ç‰‡
cv2.imwrite("face_detected.jpg", img)
```

![upgit_20250422_1745334223.png|462x616](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2025/04/upgit_20250422_1745334223.png)

### 1.6. Labï¼šMediaPipeåœ–ç‰‡äººé«”å§¿æ…‹ä¼°è¨ˆ

![upgit_20250422_1745334255.png|450x310](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2025/04/upgit_20250422_1745334255.png)

```python
import cv2
import mediapipe as mp

mp_drawing = mp.solutions.drawing_utils
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(min_detection_confidence=0.5,
                    min_tracking_confidence=0.5)
                    
img = cv2.imread("./pose01.jpg")
results = pose.process(img)
mp_drawing.draw_landmarks(
           img,
           results.pose_landmarks,
           mp_pose.POSE_CONNECTIONS)



# å„²å­˜åœ–ç‰‡
cv2.imwrite("face_detected.jpg", img)
```

![upgit_20250422_1745334386.png|233x350](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2025/04/upgit_20250422_1745334386.png)

## 2. CVZone é›»è…¦è¦–è¦ºå¥—ä»¶
ðŸ”— å®˜æ–¹ç¶²ç«™ï¼šhttps://github.com/cvzone/cvzone

CVZone æ˜¯åŸºæ–¼ OpenCV å’Œ MediaPipe çš„ Pythonå¥—ä»¶ã€‚
å¯ä»¥è®“æˆ‘å€‘ç”¨æ›´å°‘çš„ Python ç¨‹å¼ç¢¼ï¼Œå¿«é€Ÿå®Œæˆå½±åƒè™•ç†å’Œ AI é›»è…¦è¦–è¦ºåŠŸèƒ½ã€‚ 
 CVZone å¯ä»¥åšä»€éº¼ï¼Ÿ
- è‡‰éƒ¨åµæ¸¬
- 3D è‡‰éƒ¨ç¶²æ ¼å»ºç«‹
- å§¿å‹¢åµæ¸¬
- æ‰‹å‹¢è¿½è¹¤
- äººé«”å§¿æ…‹ä¼°è¨ˆ

### 2.1. å®‰è£ CVZone æ­¥é©Ÿ
è¦é…åˆmediapipeç‰ˆæœ¬
```
pip install mediapipe==0.10.14
pip install cvzone==1.6.1
```


### 2.2. Labï¼šCVZoneåµæ¸¬è‡‰éƒ¨ç¶²æ ¼

```python
from cvzone.FaceMeshModule import FaceMeshDetector
import cv2

detector = FaceMeshDetector(maxFaces=2)

img = cv2.imread("./face02.jpg")
img, faces = detector.findFaceMesh(img)
if faces:
    print(faces[0])


# å„²å­˜åœ–ç‰‡
cv2.imwrite("face_detected.jpg", img)
```

![upgit_20250423_1745412957.png|465x320](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2025/04/upgit_20250423_1745412957.png)


### 2.3. Labï¼šCVZoneåœ–ç‰‡å¤šæ‰‹å‹¢è¿½è¹¤

```python
from cvzone.HandTrackingModule import HandDetector  # åŒ¯å…¥ cvzone çš„æ‰‹éƒ¨åµæ¸¬æ¨¡çµ„
import cv2  # åŒ¯å…¥ OpenCV åšåœ–åƒè™•ç†

# å»ºç«‹æ‰‹éƒ¨åµæ¸¬å™¨ç‰©ä»¶ï¼Œè¨­å®šåµæ¸¬ä¿¡å¿ƒé–¾å€¼ç‚º 0.5ï¼Œæœ€å¤šåµæ¸¬å…©éš»æ‰‹
detector = HandDetector(detectionCon=0.5, maxHands=2)

# è®€å–ä¸€å¼µæ‰‹éƒ¨åœ–ç‰‡
img = cv2.imread("./hand.jpg")

# åŸ·è¡Œæ‰‹éƒ¨åµæ¸¬ï¼Œå›žå‚³åµæ¸¬åˆ°çš„æ‰‹éƒ¨è³‡è¨Šèˆ‡æ¨™è¨»å¾Œçš„åœ–ç‰‡
hands, img = detector.findHands(img)

# å¦‚æžœæœ‰åµæ¸¬åˆ°æ‰‹
if hands:
    # è™•ç†ç¬¬ä¸€éš»æ‰‹çš„è³‡è¨Š
    hand1 = hands[0]  # å–å‡ºç¬¬ä¸€éš»æ‰‹çš„è³‡æ–™
    lmList1 = hand1["lmList"]  # æ‰‹éƒ¨21å€‹é—œéµé»žåº§æ¨™ï¼ˆlist of x,y,zï¼‰
    bbox1 = hand1["bbox"]  # é‚Šç•Œæ¡†ï¼ˆx, y, w, hï¼‰
    centerPoint1 = hand1['center']  # ä¸­å¿ƒé»žåº§æ¨™
    handType1 = hand1["type"]  # å·¦æ‰‹æˆ–å³æ‰‹ï¼ˆ"Left" / "Right"ï¼‰
    fingers1 = detector.fingersUp(hand1)  # åˆ¤æ–·å“ªå¹¾æ ¹æ‰‹æŒ‡æ˜¯ä¼¸ç›´çš„ï¼Œå›žå‚³ä¸€å€‹5å€‹å…ƒç´ çš„åˆ—è¡¨

    # å¦‚æžœåµæ¸¬åˆ°å…©éš»æ‰‹
    if len(hands) == 2:
        hand2 = hands[1]  # ç¬¬äºŒéš»æ‰‹
        lmList2 = hand2["lmList"]
        bbox2 = hand2["bbox"]
        centerPoint2 = hand2['center']
        handType2 = hand2["type"]
        fingers2 = detector.fingersUp(hand2)

        # è¨ˆç®—å…©éš»æ‰‹çš„é£ŸæŒ‡æŒ‡å°–ï¼ˆç·¨è™Ÿ8ï¼‰ä¹‹é–“çš„è·é›¢
        length, info, img = detector.findDistance(
            lmList1[8][0:2],  # ç¬¬ä¸€éš»æ‰‹çš„é£ŸæŒ‡æŒ‡å°– (x, y)
            lmList2[8][0:2],  # ç¬¬äºŒéš»æ‰‹çš„é£ŸæŒ‡æŒ‡å°– (x, y)
            img  # å‚³å…¥åœ–åƒç”¨æ–¼ç•«ç·šå’Œæ¨™è¨˜
        )

# å°‡åµæ¸¬çµæžœçš„åœ–åƒå„²å­˜æˆæª”æ¡ˆ
cv2.imwrite("face_detected.jpg", img)
```

![upgit_20250423_1745413173.png|331x442](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2025/04/upgit_20250423_1745413173.png)


### 2.4. Labï¼šCVZone webcamå¤šæ‰‹å‹¢è¿½è¹¤

```python
from cvzone.HandTrackingModule import HandDetector
import cv2

# å•Ÿç”¨ webcamï¼ˆ0 ä»£è¡¨é è¨­æ”å½±æ©Ÿï¼‰
cap = cv2.VideoCapture(0)

# å»ºç«‹æ‰‹éƒ¨åµæ¸¬å™¨
detector = HandDetector(detectionCon=0.5, maxHands=2)

while True:
    success, img = cap.read()  # è®€å–ä¸€å¹€å½±åƒ
    if not success:
        break

    # åµæ¸¬æ‰‹éƒ¨
    hands, img = detector.findHands(img)

    # å¦‚æžœæœ‰åµæ¸¬åˆ°æ‰‹
    if hands:
        # æ‰‹ 1
        hand1 = hands[0]
        lmList1 = hand1["lmList"]
        bbox1 = hand1["bbox"]
        centerPoint1 = hand1["center"]
        handType1 = hand1["type"]
        fingers1 = detector.fingersUp(hand1)

        # å¦‚æžœæœ‰å…©éš»æ‰‹
        if len(hands) == 2:
            hand2 = hands[1]
            lmList2 = hand2["lmList"]
            bbox2 = hand2["bbox"]
            centerPoint2 = hand2["center"]
            handType2 = hand2["type"]
            fingers2 = detector.fingersUp(hand2)

            # è¨ˆç®—å…©éš»æ‰‹çš„é£ŸæŒ‡æŒ‡å°–ä¹‹é–“çš„è·é›¢
            length, info, img = detector.findDistance(
                lmList1[8][0:2], lmList2[8][0:2], img
            )

    # é¡¯ç¤ºç•«é¢
    cv2.imshow("Webcam Hand Tracking", img)

    # æŒ‰ä¸‹ q éµé›¢é–‹
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# é‡‹æ”¾è³‡æº
cap.release()
cv2.destroyAllWindows()
```



![upgit_20250423_1745413486.png|534x477](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2025/04/upgit_20250423_1745413486.png)


### 2.5. Labï¼šCVZone webcamè¨ˆç®—ä¼¸å‡ºå¹¾æ ¹æ‰‹æŒ‡
```python
from cvzone.HandTrackingModule import HandDetector
import cv2

# å•Ÿç”¨ webcamï¼Œ0 ä»£è¡¨é è¨­æ”å½±æ©Ÿ
cap = cv2.VideoCapture(1)

# å»ºç«‹æ‰‹éƒ¨åµæ¸¬å™¨ï¼Œåªåµæ¸¬ 1 éš»æ‰‹
detector = HandDetector(detectionCon=0.5, maxHands=1)

while True:
    success, img = cap.read()  # è®€å–ä¸€å¼µå½±åƒ
    if not success:
        break

    # åµæ¸¬æ‰‹éƒ¨
    hands, img = detector.findHands(img)
    
    if hands:
        hand = hands[0]  # å–å¾—ç¬¬ä¸€éš»æ‰‹çš„è³‡æ–™
        bbox = hand['bbox']  # å–å¾—æ‰‹çš„é‚Šç•Œæ¡† [x, y, w, h]
        fingers = detector.fingersUp(hand)  # åˆ¤æ–·æ‰‹æŒ‡ç‹€æ…‹ï¼Œå›žå‚³5å€‹å…ƒç´ çš„list
        totalFingers = fingers.count(1)  # è¨ˆç®—ä¼¸å‡ºçš„æ‰‹æŒ‡æ•¸é‡ï¼ˆ1ä»£è¡¨ä¼¸å‡ºï¼‰
        
        # åœ¨æ‰‹çš„ä¸Šæ–¹é¡¯ç¤ºæ‰‹æŒ‡æ•¸é‡
        if bbox:
            cv2.putText(img, f'Fingers: {totalFingers}',
                        (bbox[0] + 100, bbox[1] - 30),
                        cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 2)

    # é¡¯ç¤ºå½±åƒç•«é¢
    cv2.imshow("CVZone Hand Detection", img)

    # æŒ‰ä¸‹ q é›¢é–‹
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# é‡‹æ”¾è³‡æº
cap.release()
cv2.destroyAllWindows()
```


![upgit_20250423_1745414153.png|493x441](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2025/04/upgit_20250423_1745414153.png)


### 2.6. Labï¼šCVZone webcamè¨ˆç®—æ‰‹æŒ‡ä¹‹é–“çš„è·é›¢

```python
from cvzone.HandTrackingModule import HandDetector
import cv2

# å•Ÿç”¨ webcam
cap = cv2.VideoCapture(1)

# å»ºç«‹æ‰‹éƒ¨åµæ¸¬å™¨ï¼Œæœ€å¤šåµæ¸¬ 2 éš»æ‰‹
detector = HandDetector(detectionCon=0.5, maxHands=2)

while True:
    success, img = cap.read()
    if not success:
        break

    # åµæ¸¬æ‰‹éƒ¨
    hands, img = detector.findHands(img)

    if hands:
        # Hand 1
        hand1 = hands[0]
        lmList1 = hand1["lmList"]
        bbox1 = hand1["bbox"]
        centerPoint1 = hand1["center"]
        handType1 = hand1["type"]
        fingers1 = detector.fingersUp(hand1)

        # è¨ˆç®—å¤§æ‹‡æŒ‡ (4) å’Œé£ŸæŒ‡ (8) çš„è·é›¢
        length, info, img = detector.findDistance(
            lmList1[4][0:2], lmList1[8][0:2], img
        )

        # é¡¯ç¤ºè·é›¢
        cv2.putText(img, f'Dist: {int(length)}',
                    (bbox1[0] + 50, bbox1[1] - 30),
                    cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 2)

    # é¡¯ç¤ºç•«é¢
    cv2.imshow("CVZone Thumb-Index Distance", img)

    # æŒ‰ä¸‹ q éµé›¢é–‹
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# é‡‹æ”¾æ”å½±æ©Ÿèˆ‡é—œé–‰è¦–çª—
cap.release()
cv2.destroyAllWindows()
```

![upgit_20250423_1745420531.png|483x418](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2025/04/upgit_20250423_1745420531.png)


### 2.7. Labï¼šCVZoneåœ–ç‰‡äººé«”å§¿æ…‹ä¼°è¨ˆ

```python
from cvzone.PoseModule import PoseDetector
import cv2

detector = PoseDetector()

img = cv2.imread("./pose01.jpg")
img = detector.findPose(img)
lmList, bboxInfo = detector.findPosition(img, bboxWithHands=False)
if bboxInfo:
    center = bboxInfo["center"]
    cv2.circle(img, center, 5, (255, 0, 255), cv2.FILLED)


# å„²å­˜åœ–ç‰‡
cv2.imwrite("face_detected.jpg", img)
```

![upgit_20250423_1745420782.png](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2025/04/upgit_20250423_1745420782.png)


### 2.8. Labï¼šCVZoneåœ–ç‰‡è¨ˆç®—äººé«”å§¿æ…‹çš„å§¿æ…‹çš„è·é›¢èˆ‡è§’åº¦
```python
from cvzone.PoseModule import PoseDetector
import cv2

# å»ºç«‹å§¿å‹¢åµæ¸¬å™¨ PoseDetectorï¼ˆå…§éƒ¨åŸºæ–¼ MediaPipeï¼‰
detector = PoseDetector()

# è®€å–è¼¸å…¥åœ–åƒ
img = cv2.imread("./pose.jpg")

# åµæ¸¬äººé«”éª¨æž¶å§¿å‹¢ï¼Œæœƒç¹ªè£½éª¨æž¶åœ¨åœ–åƒä¸Š
img = detector.findPose(img)

# å–å¾—æ‰€æœ‰é—œéµé»žåº§æ¨™ï¼ˆlmListï¼‰èˆ‡æ•´é«”åŒ…è¦†æ¡† bboxInfo
lmList, bboxInfo = detector.findPosition(img, bboxWithHands=False)

if bboxInfo:
    # å¦‚æžœåµæ¸¬åˆ°èº«é«”

    # âœ³ï¸ è¨ˆç®—å·¦è‚©(11)åˆ°å·¦æ‰‹è…•(15)ä¹‹é–“çš„è·é›¢
    length, img, info = detector.findDistance(
        lmList[11][0:2],  # ç¬¬11é»žï¼šå·¦è‚© (x, y)
        lmList[15][0:2],  # ç¬¬15é»žï¼šå·¦æ‰‹è…• (x, y)
        img=img,          # åœ¨åœ–åƒä¸Šç•«ç·š
        color=(255, 0, 0), # ç·šæ¢é¡è‰²ç‚ºè—è‰² (BGR)
        scale=10          # ç·šæ¢å¯¬åº¦èˆ‡åœ“åœˆå¤§å°æ¯”ä¾‹
    )

    # âœ³ï¸ è¨ˆç®—å·¦è‚©(11)ã€å·¦æ‰‹è‚˜(13)ã€å·¦æ‰‹è…•(15)ä¹‹é–“çš„å¤¾è§’
    angle, img = detector.findAngle(
        lmList[11][0:2],  # å·¦è‚©
        lmList[13][0:2],  # å·¦æ‰‹è‚˜
        lmList[15][0:2],  # å·¦æ‰‹è…•
        img=img,          # åœ¨åœ–åƒä¸Šç•«å‡ºè§’åº¦
        color=(0, 0, 255), # ç·šæ¢é¡è‰²ç‚ºç´…è‰²
        scale=10
    )

    # âœ³ï¸ æª¢æŸ¥è§’åº¦æ˜¯å¦æŽ¥è¿‘ 50 åº¦ï¼ˆå®¹è¨±èª¤å·® Â±10ï¼‰
    isCloseAngle50 = detector.angleCheck(
        myAngle=angle,       # ç•¶å‰è§’åº¦
        targetAngle=50,      # ç›®æ¨™è§’åº¦
        offset=10            # èª¤å·®ç¯„åœ Â±10
    )

    # å°å‡ºè·é›¢å’Œè§’åº¦æ˜¯å¦æŽ¥è¿‘ç›®æ¨™è§’åº¦
    print(length, isCloseAngle50)

# å„²å­˜åŠ ä¸Šéª¨æž¶èˆ‡è§’åº¦æ¨™ç¤ºå¾Œçš„åœ–ç‰‡
cv2.imwrite("face_detected.jpg", img)

```
![upgit_20250423_1745423563.png](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2025/04/upgit_20250423_1745423563.png)



### 2.9. Labï¼šå¾ž 2D åœ–ç‰‡åµæ¸¬äººé«”å§¿å‹¢ â†’ è½‰ç‚º 3D éª¨æž¶
å…±å…©å€‹æª”æ¡ˆ
```python
# Pose3D.py
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np

def plotPose3D(lmList):
    face_index_list = [8, 6, 5, 4, 0, 1, 2, 3, 7]
    mouth_index_list = [9, 10]
    right_arm_index_list = [11, 13, 15, 17, 19, 15, 21]
    left_arm_index_list = [12, 14, 16, 18, 20, 16, 22]
    right_body_side_index_list = [11, 23, 25, 27, 29, 31, 27]
    left_body_side_index_list = [12, 24, 26, 28, 30, 32, 28]
    shoulder_index_list = [11, 12]
    waist_index_list = [23, 24]

    # çœ¼è€³é¼» - è‡‰éƒ¨
    face_x, face_y, face_z = [], [], []
    for index in face_index_list:
        point = lmList[index]
        face_x.append(point[0])
        face_y.append(point[1])
        face_z.append(point[2])
    
    # å˜´å·´   
    mouth_x, mouth_y, mouth_z = [], [], []
    for index in mouth_index_list:
        point = lmList[index]
        mouth_x.append(point[0])
        mouth_y.append(point[1])
        mouth_z.append(point[2])
    
    # å³æ‰‹
    right_arm_x, right_arm_y, right_arm_z = [], [], []
    for index in right_arm_index_list:
        point = lmList[index]
        right_arm_x.append(point[0])
        right_arm_y.append(point[1])
        right_arm_z.append(point[2])

    # å·¦æ‰‹
    left_arm_x, left_arm_y, left_arm_z = [], [], []
    for index in left_arm_index_list:
        point = lmList[index]
        left_arm_x.append(point[0])
        left_arm_y.append(point[1])
        left_arm_z.append(point[2])

    # å³åŠèº«
    right_body_side_x, right_body_side_y, right_body_side_z = [], [], []
    for index in right_body_side_index_list:
        point = lmList[index]
        right_body_side_x.append(point[0])
        right_body_side_y.append(point[1])
        right_body_side_z.append(point[2])

    # å·¦åŠèº«
    left_body_side_x, left_body_side_y, left_body_side_z = [], [], []
    for index in left_body_side_index_list:
        point = lmList[index]
        left_body_side_x.append(point[0])
        left_body_side_y.append(point[1])
        left_body_side_z.append(point[2])

    # è‚©
    shoulder_x, shoulder_y, shoulder_z = [], [], []
    for index in shoulder_index_list:
        point = lmList[index]
        shoulder_x.append(point[0])
        shoulder_y.append(point[1])
        shoulder_z.append(point[2])

    # è…°
    waist_x, waist_y, waist_z = [], [], []
    for index in waist_index_list:
        point = lmList[index]
        waist_x.append(point[0])
        waist_y.append(point[1])
        waist_z.append(point[2])
            
    fig = plt.figure()
    ax = fig.add_subplot(projection='3d')
    ax.set_xlabel('X Label')
    ax.set_ylabel('Y Label')
    ax.set_zlabel('Z Label')

    ax.scatter(face_x, face_y, face_z)
    ax.plot(mouth_x, mouth_y, mouth_z)
    ax.plot(right_arm_x, right_arm_y, right_arm_z)
    ax.plot(left_arm_x, left_arm_y, left_arm_z)
    ax.plot(right_body_side_x, right_body_side_y, right_body_side_z)
    ax.plot(left_body_side_x, left_body_side_y, left_body_side_z)
    ax.plot(shoulder_x, shoulder_y, shoulder_z)
    ax.plot(waist_x, waist_y, waist_z)

    ax.view_init(elev=300, azim=330, roll=300)   # æŒ‡å®šè¦–è§’
    ax.set_box_aspect([1, 1, 1])                 # æŒ‡å®šx, y, zçš„æ¯”ä¾‹
    # plt.savefig("pose_landmark.png") 
    plt.show()
```

```python
# main.py
from cvzone.PoseModule import PoseDetector
from Pose3D import plotPose3D
import cv2

detector = PoseDetector()

img = cv2.imread("./pose.jpg")
img = detector.findPose(img)
lmList, bboxInfo = detector.findPosition(img, draw=False, bboxWithHands=False)
if lmList:
    print("lmList =", lmList)
    for index, lm in enumerate(lmList):
        print(index, lm)
    plotPose3D(lmList) 
    
```



![upgit_20250424_1745424263.png](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2025/04/upgit_20250424_1745424263.png)


### 2.10. AIè¾¨è­˜çŒœæ‹³
å…±å…©å€‹æª”æ¡ˆ

```python
# HandTrackingModule.py
"""
Hand Tracking Module
By: Computer Vision Zone
Website: https://www.computervision.zone/
"""

import math

import cv2
import mediapipe as mp


class HandDetector:
    """
    Finds Hands using the mediapipe library. Exports the landmarks
    in pixel format. Adds extra functionalities like finding how
    many fingers are up or the distance between two fingers. Also
    provides bounding box info of the hand found.
    """

    def __init__(self, staticMode=False, maxHands=2, modelComplexity=1, detectionCon=0.5, minTrackCon=0.5):

        """
        :param mode: In static mode, detection is done on each image: slower
        :param maxHands: Maximum number of hands to detect
        :param modelComplexity: Complexity of the hand landmark model: 0 or 1.
        :param detectionCon: Minimum Detection Confidence Threshold
        :param minTrackCon: Minimum Tracking Confidence Threshold
        """
        self.staticMode = staticMode
        self.maxHands = maxHands
        self.modelComplexity = modelComplexity
        self.detectionCon = detectionCon
        self.minTrackCon = minTrackCon
        self.mpHands = mp.solutions.hands
        self.hands = self.mpHands.Hands(static_image_mode=self.staticMode,
                                        max_num_hands=self.maxHands,
                                        model_complexity=modelComplexity,
                                        min_detection_confidence=self.detectionCon,
                                        min_tracking_confidence=self.minTrackCon)

        self.mpDraw = mp.solutions.drawing_utils
        self.tipIds = [4, 8, 12, 16, 20]
        self.fingers = []
        self.lmList = []

    def findHands(self, img, draw=True, flipType=True):
        """
        Finds hands in a BGR image.
        :param img: Image to find the hands in.
        :param draw: Flag to draw the output on the image.
        :return: Image with or without drawings
        """
        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        self.results = self.hands.process(imgRGB)
        allHands = []
        h, w, c = img.shape
        if self.results.multi_hand_landmarks:
            for handType, handLms in zip(self.results.multi_handedness, self.results.multi_hand_landmarks):
                myHand = {}
                ## lmList
                mylmList = []
                xList = []
                yList = []
                for id, lm in enumerate(handLms.landmark):
                    px, py, pz = int(lm.x * w), int(lm.y * h), int(lm.z * w)
                    mylmList.append([px, py, pz])
                    xList.append(px)
                    yList.append(py)

                ## bbox
                xmin, xmax = min(xList), max(xList)
                ymin, ymax = min(yList), max(yList)
                boxW, boxH = xmax - xmin, ymax - ymin
                bbox = xmin, ymin, boxW, boxH
                cx, cy = bbox[0] + (bbox[2] // 2), \
                         bbox[1] + (bbox[3] // 2)

                myHand["lmList"] = mylmList
                myHand["bbox"] = bbox
                myHand["center"] = (cx, cy)

                if flipType:
                    if handType.classification[0].label == "Right":
                        myHand["type"] = "Left"
                    else:
                        myHand["type"] = "Right"
                else:
                    myHand["type"] = handType.classification[0].label
                allHands.append(myHand)

                ## draw
                if draw:
                    self.mpDraw.draw_landmarks(img, handLms,
                                               self.mpHands.HAND_CONNECTIONS)
                    cv2.rectangle(img, (bbox[0] - 20, bbox[1] - 20),
                                  (bbox[0] + bbox[2] + 20, bbox[1] + bbox[3] + 20),
                                  (255, 0, 255), 2)
                    cv2.putText(img, myHand["type"], (bbox[0] - 30, bbox[1] - 30), cv2.FONT_HERSHEY_PLAIN,
                                2, (255, 0, 255), 2)

        return allHands, img

    def fingersUp(self, myHand):
        """
        Finds how many fingers are open and returns in a list.
        Considers left and right hands separately
        :return: List of which fingers are up
        """
        fingers = []
        myHandType = myHand["type"]
        myLmList = myHand["lmList"]
        if self.results.multi_hand_landmarks:

            # Thumb
            if myHandType == "Right":
                if myLmList[self.tipIds[0]][0] > myLmList[self.tipIds[0] - 1][0]:
                    fingers.append(1)
                else:
                    fingers.append(0)
            else:
                if myLmList[self.tipIds[0]][0] < myLmList[self.tipIds[0] - 1][0]:
                    fingers.append(1)
                else:
                    fingers.append(0)

            # 4 Fingers
            for id in range(1, 5):
                if myLmList[self.tipIds[id]][1] < myLmList[self.tipIds[id] - 2][1]:
                    fingers.append(1)
                else:
                    fingers.append(0)
        return fingers

    def findDistance(self, p1, p2, img=None, color=(255, 0, 255), scale=5):
        """
        Find the distance between two landmarks input should be (x1,y1) (x2,y2)
        :param p1: Point1 (x1,y1)
        :param p2: Point2 (x2,y2)
        :param img: Image to draw output on. If no image input output img is None
        :return: Distance between the points
                 Image with output drawn
                 Line information
        """
        if len(p1) == len(p2) == 3:
            # Get the landmarks
            x1, y1 = p1[0:2]
            x2, y2 = p2[0:2]
        else:
            x1, y1 = p1
            x2, y2 = p2

        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2
        length = math.hypot(x2 - x1, y2 - y1)
        info = (x1, y1, x2, y2, cx, cy)
        if img is not None:
            cv2.circle(img, (x1, y1), scale, color, cv2.FILLED)
            cv2.circle(img, (x2, y2), scale, color, cv2.FILLED)
            cv2.line(img, (x1, y1), (x2, y2), color, max(1, scale // 3))
            cv2.circle(img, (cx, cy), scale, color, cv2.FILLED)

        return length, img, info

    def findDistance3D(self, p1, p2, img=None, color=(255, 0, 255), scale=5):
        """
           Find the distance between two landmarks input should be (x1,y1,z1) (x2,y2,z2)
           :param p1: Point1 (x1,y1,z1)
           :param p2: Point2 (x2,y2,z2)
           :param img: Image to draw output on. If no image input output img is None
           :return: Distance between the points
                    Image with output drawn
                    Line information
           """
        x1, y1, z1 = p1
        x2, y2, z2 = p2
        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2
        length = math.sqrt((x2 - x1)**2 + (y2 - y1)**2 + (z2 - z1)**2)
        info = (x1, y1, x2, y2, cx, cy)

        if img is not None:
            cv2.line(img, (x1, y1), (x2, y2), color, max(1, scale // 3))
            cv2.circle(img, (x1, y1), scale, color, cv2.FILLED)
            cv2.circle(img, (x2, y2), scale, color, cv2.FILLED)
            cv2.circle(img, (cx, cy), scale, color, cv2.FILLED)

        return length, img, info

    def findAngle(self,p1, p2, p3, img=None, color=(255, 0, 255), scale=5):
        """
        Finds angle between three points.

        :param p1: Point1 - (x1,y1,z1)
        :param p2: Point2 - (x2,y2,z2)
        :param p3: Point3 - (x3,y3,z3)
        :param img: Image to draw output on. If no image input output img is None
        :return:
        """
        angle, img = self.findAngle3D(p1[0:2], p2[0:2], p3[0:2],
                                      img=img,
                                      color=color,
                                      scale=scale)
        return angle, img

    def findAngle3D(self, point_a, point_b, point_c, img=None, color=(255, 0, 255), scale=5):
        """
        Finds angle between three points.

        :param point_a: Point1 - (x1,y1,z1)
        :param point_b: Point2 - (x2,y2,z2)
        :param point_c: Point3 - (x3,y3,z3)
        :param img: Image to draw output on. If no image input output img is None
        :return:   
        """
        a_x, b_x, c_x = point_a[0], point_b[0], point_c[0]  # aã€bã€cä¸‰å€‹é»žçš„xåº§æ¨™
        a_y, b_y, c_y = point_a[1], point_b[1], point_c[1]  # aã€bã€cä¸‰å€‹é»žçš„yåº§æ¨™

        if len(point_a) == len(point_b) == len(point_c) == 3:
            a_z, b_z, c_z = point_a[2], point_b[2], point_c[2]  # aã€bã€cä¸‰å€‹é»žçš„zåº§æ¨™
        else:
            a_z, b_z, c_z = 0,0,0  # å› ç‚ºæ˜¯äºŒç¶­åº§æ¨™, æ‰€ä»¥zè»¸éƒ½ç‚º0

        # å‘é‡ m=(x1,y1,z1), n=(x2,y2,z2)
        x1,y1,z1 = (a_x-b_x),(a_y-b_y),(a_z-b_z)
        x2,y2,z2 = (c_x-b_x),(c_y-b_y),(c_z-b_z)

        # è¨ˆç®—è§’åº¦
        cos_b = (x1*x2 + y1*y2 + z1*z2) / (math.sqrt(x1**2 + y1**2 + z1**2) *(math.sqrt(x2**2 + y2**2 + z2**2))) 
        angle = math.degrees(math.acos(cos_b))
        
        # Draw
        if img is not None:
            cv2.line(img, (a_x, a_y), (b_x, b_y), (255, 255, 255), max(1,scale//5))
            cv2.line(img, (c_x, c_y), (b_x, b_y), (255, 255, 255), max(1,scale//5))
            cv2.circle(img, (a_x, a_y), scale, color, cv2.FILLED)
            cv2.circle(img, (a_x, a_y), scale+5, color, max(1,scale//5))
            cv2.circle(img, (b_x, b_y), scale, color, cv2.FILLED)
            cv2.circle(img, (b_x, b_y), scale+5, color, max(1,scale//5))
            cv2.circle(img, (c_x, c_y), scale, color, cv2.FILLED)
            cv2.circle(img, (c_x, c_y), scale+5, color, max(1,scale//5))
            cv2.putText(img, str(int(angle)), (b_x - 50, b_y + 50),
                        cv2.FONT_HERSHEY_PLAIN, 2, color, max(1,scale//5))
        
        return angle, img

    def angleCheck(self, myAngle, targetAngle, offset=20):
        return targetAngle - offset < myAngle < targetAngle + offset


def main():
    # Initialize the webcam to capture video
    # The '2' indicates the third camera connected to your computer; '0' would usually refer to the built-in camera
    cap = cv2.VideoCapture(0)

    # Initialize the HandDetector class with the given parameters
    detector = HandDetector(staticMode=False, maxHands=2, modelComplexity=1, detectionCon=0.5, minTrackCon=0.5)

    # Continuously get frames from the webcam
    while True:
        # Capture each frame from the webcam
        # 'success' will be True if the frame is successfully captured, 'img' will contain the frame
        success, img = cap.read()

        # Find hands in the current frame
        # The 'draw' parameter draws landmarks and hand outlines on the image if set to True
        # The 'flipType' parameter flips the image, making it easier for some detections
        hands, img = detector.findHands(img, draw=True, flipType=True)

        # Check if any hands are detected
        if hands:
            # Information for the first hand detected
            hand1 = hands[0]  # Get the first hand detected
            lmList1 = hand1["lmList"]  # List of 21 landmarks for the first hand
            bbox1 = hand1["bbox"]  # Bounding box around the first hand (x,y,w,h coordinates)
            center1 = hand1['center']  # Center coordinates of the first hand
            handType1 = hand1["type"]  # Type of the first hand ("Left" or "Right")

            # Count the number of fingers up for the first hand
            fingers1 = detector.fingersUp(hand1)
            print(f'H1 = {fingers1.count(1)}', end=" ")  # Print the count of fingers that are up

            # Calculate distance between specific landmarks on the first hand and draw it on the image
            length, info, img = detector.findDistance(lmList1[8][0:2], lmList1[12][0:2], img, color=(255, 0, 255),
                                                      scale=10)

            # Check if a second hand is detected
            if len(hands) == 2:
                # Information for the second hand
                hand2 = hands[1]
                lmList2 = hand2["lmList"]
                bbox2 = hand2["bbox"]
                center2 = hand2['center']
                handType2 = hand2["type"]

                # Count the number of fingers up for the second hand
                fingers2 = detector.fingersUp(hand2)
                print(f'H2 = {fingers2.count(1)}', end=" ")

                # Calculate distance between the index fingers of both hands and draw it on the image
                length, info, img = detector.findDistance(lmList1[8][0:2], lmList2[8][0:2], img, color=(255, 0, 0),
                                                          scale=10)

            print(" ")  # New line for better readability of the printed output

        # Display the image in a window
        cv2.imshow("Image", img)

        # Keep the window open and update it for each frame; wait for 1 millisecond between frames
        cv2.waitKey(1)


if __name__ == "__main__":
    main()
```

```python
# main.py
from cvzone.HandTrackingModule import HandDetector
import cv2

cap = cv2.VideoCapture(1)
detector = HandDetector(detectionCon=0.5, maxHands=1)

while cap.isOpened():
    success, img = cap.read()
    hands, img = detector.findHands(img)
    if hands:
        hand = hands[0]
        bbox = hand["bbox"]        
        fingers = detector.fingersUp(hand)
        totalFingers = fingers.count(1)
        print(totalFingers)
        msg = "None"
        if totalFingers == 5:
            msg = "Paper"
        if totalFingers == 0:
            msg = "Rock"
        if totalFingers == 2:
            if fingers[1] == 1 and fingers[2] == 1:
                msg = "Scissors"
        cv2.putText(img, msg, (bbox[0]+200,bbox[1]-30),
                    cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 2)
    cv2.imshow("CVZone Hand Detector", img)
    if cv2.waitKey(1) & 0xFF == ord("q"):
        break
        
cap.release()
cv2.destroyAllWindows()
```


![upgit_20250424_1745425150.png](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2025/04/upgit_20250424_1745425150.png)
